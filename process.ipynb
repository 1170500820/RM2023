{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f4e5d333-9056-4d31-b577-f5f5505dbd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse import csr_matrix\n",
    "import json\n",
    "from pandas import Series, DataFrame\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import matplotlib\n",
    "matplotlib.style.use('ggplot')\n",
    "pd.set_option('max_colwidth',50)\n",
    "import torch\n",
    "from collections import Counter\n",
    "\n",
    "from loguru import logger"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e424623-3f41-4778-868f-f5f6393eb903",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 数据的预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e90cb3c6-1f7c-4661-9d0e-5ba306221259",
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = 'data/processed/train.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "33a67b36-20b8-48e1-8b36-d82db5359642",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = list(json.loads(x) for x in open(fname, 'r', encoding='utf-8').read().strip().split('\\n'))\n",
    "df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dd3e867-46e9-46c0-b223-8082c7ac6624",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 寻找共现标签并删除"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d06be8-871d-4f2b-9058-0c54344fab19",
   "metadata": {},
   "source": [
    "### 寻找共现标签"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55bf5e8f-c448-45fa-991f-bc0da02a3ad5",
   "metadata": {},
   "source": [
    "##### 读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c9743529-0113-41e4-beb5-89b14d249066",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = list(json.loads(x) for x in open(fname, 'r', encoding='utf-8').read().strip().split('\\n'))\n",
    "df = pd.DataFrame(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e634d9a6-bdef-44c7-93f4-84066ec1d03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "b = df['label'].apply(lambda x: 'label_123' in x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "06ef0f0a-da3c-4f29-9040-89c2af2d7cc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.to_numpy().astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f3d352b-b0e1-4d93-b648-8232b2120e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1399/1399 [00:51<00:00, 27.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# 获取所有label的set\n",
    "label_set = set()\n",
    "for e in df['label']:\n",
    "    for e_label in e: label_set.add(e_label)\n",
    "\n",
    "# 每一个label在句子中出现的布尔向量\n",
    "# 将布尔向量改为10，作为二进制数转化为int，作为该label的fingerprint\n",
    "s = df.shape[0]\n",
    "label2binary = {}  # label -> fingerprint\n",
    "fp2label = {}\n",
    "for e in tqdm(label_set):\n",
    "    pos_bool = df['label'].apply(lambda x: e in x).to_numpy().astype(int).tolist()\n",
    "    fingerprint = int(''.join(list(str(x) for x in pos_bool)))\n",
    "    if fingerprint not in fp2label:\n",
    "        fp2label[fingerprint] = [e]\n",
    "    else:\n",
    "        fp2label[fingerprint].append(e)\n",
    "\n",
    "# 找到fingerprint相同的label\n",
    "same_groups = []\n",
    "for v in fp2label.values():\n",
    "    if len(v) >= 2:\n",
    "        same_groups.append(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9b5e63d8-c2a4-4049-9230-ee85e43b47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "label2equivalent = {}\n",
    "for e in same_groups:\n",
    "    label2equivalent[e[0]] = e[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb2c5965-5900-435e-a8eb-e2ea34b0aaf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(same_groups, open('temp_data/equivalent_labels.json', 'w', encoding='utf-8'))\n",
    "json.dump(label2equivalent, open('temp_data/label2equivalent.json', 'w', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be2ea9-ce37-4329-abe0-0888309f962d",
   "metadata": {},
   "source": [
    "### 读取共现标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ff85e300-be1b-4292-a726-9cc729dd9923",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_groups = json.load(open('temp_data/equivalent_labels.json', 'r', encoding='utf-8'))\n",
    "label2equivalent = json.load(open('temp_data/label2equivalent.json', 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05982689-b351-402a-9edd-199c593d8fc7",
   "metadata": {},
   "source": [
    "### 删除df中的一致标签"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "19ff9389-770e-4450-88f5-123410e4ab55",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_set = set()\n",
    "for e in same_groups:\n",
    "    for d in e[1:]:\n",
    "        delete_set.add(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53b27e8d-c406-49ee-b79d-7f0876710a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_list(l):\n",
    "    new_list = []\n",
    "    for e in l:\n",
    "        if e not in delete_set:\n",
    "            new_list.append(e)\n",
    "    return new_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ade7ce14-0349-4bd6-aded-8597629797e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = df.copy()\n",
    "filtered_df['label'] = filtered_df['label'].apply(update_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "49718f9e-fed3-4daf-8f23-d4dd97ad363f",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df.to_pickle('temp_data/filtered_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edafa9ca-669e-4df9-98eb-77767029315a",
   "metadata": {},
   "source": [
    "### 读取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5d81da26-a5d4-47bc-9f35-d1ef001e2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = pd.read_pickle('temp_data/filtered_df.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e39a1b6-2a0e-4b03-91b7-5190f9e5f5ac",
   "metadata": {},
   "source": [
    "## 分词"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2230594c-2e2a-4ecb-b93c-0fc74a4ad085",
   "metadata": {},
   "source": [
    "### 直接分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0176a92b-63c2-4621-b88c-227b3b829b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c2b591fc-fdbc-47f3-89a7-b7e5e932b282",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/v2/_14t7h052kgc9sf4crk2qb000000gn/T/jieba.cache\n",
      "Loading model cost 0.430 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "df_splitted = filtered_df.copy()\n",
    "df_splitted['splitted'] = df_splitted['text'].apply(lambda x: list(jieba.cut(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4dafbbd9-bc79-4ba8-90df-12cb9aeda538",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_splitted.to_pickle('temp_data/df_splitted.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98eebdcd-3193-49b2-a30a-4bdfedbd4bc9",
   "metadata": {},
   "source": [
    "### 构造词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "59ae59f9-3285-44b5-b566-8be4b1279c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77314/77314 [01:24<00:00, 913.20it/s] \n"
     ]
    }
   ],
   "source": [
    "vocabulary = set()\n",
    "for e in tqdm(df_splitted['splitted']):\n",
    "    vocabulary = vocabulary.union(set(e))\n",
    "vocabulary = sorted(list(vocabulary))\n",
    "vocabulary_idx = {x: i for i, x in enumerate(vocabulary)}\n",
    "json.dump(vocabulary, open('temp_data/vocabulary.json', 'w', encoding='utf-8'))\n",
    "json.dump(vocabulary_idx, open('temp_data/vocabulary_idx.json', 'w', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6125816-aa26-47c6-ba20-42dec04010bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = json.load(open('temp_data/vocabulary.json', 'r', encoding='utf-8'))\n",
    "vocabulary_idx = json.load(open('temp_data/vocabulary_idx.json', 'r', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2e15bd-b814-45ec-a10b-6da7bba07b93",
   "metadata": {},
   "source": [
    "### 构造词语在句子中的出现矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4558e395-f95d-4386-8005-0aa10432d4fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['这个',\n",
       " '七星',\n",
       " '连珠',\n",
       " '的',\n",
       " '现象',\n",
       " '居然',\n",
       " '真的',\n",
       " '存在',\n",
       " '，',\n",
       " '我',\n",
       " '只',\n",
       " '在',\n",
       " '小说',\n",
       " '上面',\n",
       " '看过',\n",
       " '，',\n",
       " '太',\n",
       " '神奇',\n",
       " '了',\n",
       " '吧',\n",
       " '，',\n",
       " '至于',\n",
       " '其他',\n",
       " '的',\n",
       " '我',\n",
       " '也',\n",
       " '不',\n",
       " '太',\n",
       " '懂',\n",
       " '，',\n",
       " '科学',\n",
       " '现象',\n",
       " '还是',\n",
       " '需要',\n",
       " '很多',\n",
       " '依据',\n",
       " '证明',\n",
       " '的']"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_splitted['splitted'][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6102b0c-220c-4d07-a1a9-9f4ab6051f93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77314it [00:01, 61839.15it/s]\n"
     ]
    }
   ],
   "source": [
    "# shape=(vocab, sentence cnt)，词语在句子中的出现矩阵\n",
    "vocab_appear = np.zeros((len(vocabulary), df_splitted.shape[0]), dtype=bool)\n",
    "for i, row in tqdm(enumerate(df_splitted['splitted'])):\n",
    "    for w in row:\n",
    "        w_idx = vocabulary_idx[w]\n",
    "        vocab_appear[w_idx][i] = True\n",
    "csr_vocab_appear = csr_matrix(vocab_appear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0f1fa209-0e87-4e9d-97ed-cc26ba4bc117",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('temp_data/csr_vocab_appear', csr_vocab_appear)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d589a8d-49fe-4428-80fb-be3fef53e52a",
   "metadata": {},
   "source": [
    "# 关键词方案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1ac11ee0-c168-430d-a5b4-76bede192534",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Object arrays cannot be loaded when allow_pickle=False",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [56]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m csr_vocab_appear \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtemp_data/csr_vocab_appear.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m vocab_appear \u001b[38;5;241m=\u001b[39m csr_vocab_appear\u001b[38;5;241m.\u001b[39mtodense()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/numpy/lib/npyio.py:440\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    438\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode)\n\u001b[1;32m    439\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 440\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    441\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/pytorch/lib/python3.8/site-packages/numpy/lib/format.py:743\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mhasobject:\n\u001b[1;32m    741\u001b[0m     \u001b[38;5;66;03m# The array contained Python objects. We need to unpickle the data.\u001b[39;00m\n\u001b[1;32m    742\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n\u001b[0;32m--> 743\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mObject arrays cannot be loaded when \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    744\u001b[0m                          \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_pickle=False\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    745\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m pickle_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    746\u001b[0m         pickle_kwargs \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[0;31mValueError\u001b[0m: Object arrays cannot be loaded when allow_pickle=False"
     ]
    }
   ],
   "source": [
    "csr_vocab_appear = np.load('temp_data/csr_vocab_appear.npy', allow_pickle=True)\n",
    "vocab_appear = csr_vocab_appear.todense()\n",
    "df_splitted = pd.read_pickle('temp_data/df_splitted.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df163bed-f44c-4023-bd3f-f6e6650987cf",
   "metadata": {},
   "source": [
    "## 构造标签矩阵"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33863e50-3f8f-4985-8a0b-f1e1dec052f5",
   "metadata": {},
   "source": [
    "### 标签表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "23a88faf-d304-48e1-8c4c-8f7ffb223bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77314/77314 [00:00<00:00, 1561049.72it/s]\n"
     ]
    }
   ],
   "source": [
    "label_vocabulary = []\n",
    "for e in tqdm(df_splitted['label']):\n",
    "    label_vocabulary.extend(e)\n",
    "label_count = sorted(list(Counter(label_vocabulary).items()))\n",
    "label_vocabulary = list(x[0] for x in label_count)\n",
    "label_count = list(x[1] for x in label_count)\n",
    "label_vocabulary_idx = {x: i for i, x in enumerate(label_vocabulary)}\n",
    "\n",
    "json.dump(label_vocabulary, open('temp_data/label_vocabulary.json', 'w', encoding='utf-8'))\n",
    "json.dump(label_vocabulary_idx, open('temp_data/label_vocabulary_idx.json', 'w', encoding='utf-8'))\n",
    "json.dump(label_count, open('temp_data/label_count.json', 'w', encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55e17fc-2610-4a6f-ab03-b25cae481f6a",
   "metadata": {},
   "source": [
    "### 标签在句子中的出现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1354138c-8d92-4b6b-b177-e039e9c19f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "77314it [00:00, 897204.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# shape=(label_vocab, sentence cnt) 标签在句子中的出现矩阵\n",
    "label_appear = np.zeros((len(label_vocabulary), df_splitted.shape[0]), dtype=bool)\n",
    "for i, row in tqdm(enumerate(df_splitted['label'])):\n",
    "    for l in row:\n",
    "        l_idx = label_vocabulary_idx[l]\n",
    "        label_appear[l_idx][i] = True\n",
    "csr_label_appear = csr_matrix(label_appear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e297f37b-fb31-49df-acde-23c55fa79b65",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('temp_data/csr_label_appear', csr_label_appear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "162e0ebe-7c0d-49c4-bb77-b2f490f9e00a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((87875, 77314), (1023, 77314))"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_appear.shape, label_appear.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb33c703-d82f-40fd-ad41-4eb3ded310ea",
   "metadata": {},
   "source": [
    "### 验证标签矩阵和词矩阵的正确和一致性"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7dd2c-da55-4258-8b1a-3223cf97d7d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 365590/2012049 [00:01<00:06, 259624.31it/s]"
     ]
    }
   ],
   "source": [
    "vocab_info = np.where(vocab_appear)\n",
    "v_idx, s_idx = vocab_info[0], vocab_info[1]\n",
    "for elem_vidx, elem_sidx in tqdm(list(zip(v_idx, s_idx))):\n",
    "    cur_word = vocabulary[elem_vidx]\n",
    "    assert cur_word in df_splitted['splitted'][elem_sidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf38a1ae-e373-414d-8d5e-951ec34febb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_info = np.where(label_appear)\n",
    "l_idx, s_idx = label_info[0], label_info[1]\n",
    "for elem_lidx, elem_sidx in tqdm(list(zip(l_idx, s_idx))):\n",
    "    cur_label = label_vocabulary[elem_lidx]\n",
    "    assert cur_label in df_splitted['label'][elem_sidx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e23c0-6068-44a5-a8cc-a24c920f61ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def frag_matmul(a, b, max_dim=100, verbose=False, use_cuda=False):\n",
    "    # a = (a1, m), b = (m, b2)\n",
    "    # 将a1和a2化为多个长为100的block\n",
    "    a_rows, b_columns = [], []\n",
    "    \n",
    "    if verbose:\n",
    "        logger.info('正在碎片化')\n",
    "    dim_a, dim_b = a.shape[0], b.shape[1]\n",
    "    a_frag_cnt = (dim_a + max_dim - 1) // max_dim\n",
    "    b_frag_cnt = (dim_b + max_dim - 1) // max_dim\n",
    "    for ia in range(a_frag_cnt):\n",
    "        a_rows.append(a[ia * max_dim: min((ia + 1) * max_dim, a.shape[0])])\n",
    "    for ib in range(b_frag_cnt):\n",
    "        b_columns.append(b[:, ib * max_dim: min((ib + 1) * max_dim, b.shape[1])])\n",
    "    \n",
    "    results = []\n",
    "    if verbose:\n",
    "        logger.info('正在将碎片进行矩阵乘法')\n",
    "    if use_cuda:\n",
    "        for i_r in tqdm(range(a_frag_cnt)):\n",
    "            results.append([])\n",
    "            for i_c in range(b_frag_cnt):\n",
    "                cur_a_row = torch.tensor(a_rows[i_r]).to(torch.float).to('cuda')\n",
    "                cur_b_column = torch.tensor(b_columns[i_c]).to(torch.float).to('cuda')\n",
    "                res = torch.matmul(cur_a_row, cur_b_column)\n",
    "                results[-1].append(np.array(res.cpu(), dtype=int))\n",
    "    else:\n",
    "        for i_r in tqdm(range(a_frag_cnt)):\n",
    "            results.append([])\n",
    "            for i_c in range(b_frag_cnt):\n",
    "                results[-1].append(np.matmul(a_rows[i_r], b_columns[i_c]))\n",
    "    \n",
    "    \n",
    "    if verbose: logger.info('正在合并碎片')\n",
    "    np_rows = []\n",
    "    for erow in tqdm(results):\n",
    "        np_rows.append(np.concatenate(erow, axis=1))\n",
    "    np_result = np.concatenate(np_rows)\n",
    "    return np_result\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd659cd8-9772-473b-a878-4f0958694b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.random.rand(100, 200)\n",
    "b = np.random.rand(200, 100)\n",
    "c = np.matmul(a, b)\n",
    "d = frag_matmul(a, b, max_dim=30)\n",
    "(c != d).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980087b8-762c-4f41-b1c8-26564d53ab75",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
